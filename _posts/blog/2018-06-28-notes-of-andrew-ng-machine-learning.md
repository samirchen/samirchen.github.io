---
layout: post
title: Andrew Ng 机器学习课程笔记
description: 机器学习入门学习。
category: blog
tag: Machine Learning
---

更详细的笔记参见：

[Coursera 机器学习笔记](http://daniellaah.github.io/2016/Machine-Learning-Andrew-Ng-My-Notes.html)


这里只记一些偏概念性的笔记便于快速理解。


Week 1: Introduction


## 机器学习的定义

 一个程序被认为能从经验 E 中学习，解决任务 T，达到性能度量值 P，当且仅当，有了经验 E 后，经过 P 评判， 程序在处理 T 时的性能有所提升。

以下西洋棋为例，经验 e 就是程序上万次的自我练习的经验，任务 t 就是下棋，性能度量值 p 就是它在与一些新的对手比赛时赢得比赛的概率。


## 机器学习算法

最常用的两种机器学习算法：

- 监督学习(Supervised Learning)
- 无监督学习(Unsupervised Learning)

此外，还有强化学习(Reinforcement Learning)和推荐系统(Recommender Systems)。







## 监督学习算法

在监督学习中，我们已经有一个给定的数据集，并且对于给定的输入已经知道正确的输出是什么，而且也确定输入和输出之间存在关系。

监督学习问题分为「回归(Regression)」和「分类(Classification)」问题。在回归问题中，我们试图预测连续输出中的结果，这意味着我们试图将输入变量映射到某个连续函数。在分类问题中，我们试图预测离散输出中的结果。换句话说，我们试图将输入变量映射到离散类别。

比如，给定一张人的照片，我们根据给定的图片预测他们的年龄，这是一个「回归(Regression)」问题。对于一个肿瘤患者，我们来预测肿瘤是恶性的还是良性的，这是一个「分类(Classification)」问题。






## 无监督学习

在无监督学习中，我们并不清楚问题的结果是什么样，但是我们可以从数据中得出结构，并且我们不一定知道变量的影响。

我们可以通过基于数据中变量之间的关系对数据进行「聚类(Clustering)」来推导出这种结构。在无监督学习的情况下，没有基于预测结果的反馈。

比如，拿 100 万个不同基因的集合，并找到一种方法将这些基因自动分组成在不同变量维度上相似或相关的组，例如寿命，位置，角色等，这就是一个聚类问题。

也有非聚类问题，比如混合音频的分离。



## 假设函数、模型、代价函数

训练集(Training Set)，是由训练样本组成的数据集合。


假设函数(Hypothesis Function)，使用某种学习算法对训练集的数据进行训练，我们可以得到假设函数。在房价预测的例子中，假设函数就是一个房价关于房子面积的函数。有了这个假设函数之后，给定一个房子的面积我们就可以预测它的价格了。


在房价预测的例子中，我们使用的模型叫单变量的线性回归模型(Linear Regression with One Variable/Univariate Linear Regression)。


当我们得到了假设函数，我们就可以进行预测了，但是假设函数中有一些系数，当选择不同的系数时，我们得到的模型效果就会不一样。那么现在的问题就是该如何选择这些系数。我们的目标是得到对应的系数，使得对于训练样例 (x, y) 得到的 h(x) 最接近 y，越是接近（通常我们会选择使用 y 和 h(x) 的均方差来作为衡量标准），表示这个假设函数越是准确。而代价函数(Cost Function)则是以假设函数中的系数作为变量来描述 y 和 h(x) 接近程度的函数。那么上面的问题就转化为了：找出使得代价函数得到最小值的系数。


「梯度下降算法」是一种帮助我们找到一个函数的局部极小值点的算法，它不仅仅可以用在线性回归模型中，在机器学习许多其他的模型中也可以使用它。

对于单变量线性回归来说，我们可以使用梯度下降算法来找到最优的系数。梯度下降的过程是对代价函数的每个 θ 求导数，梯度则是对应点的切线的斜率，我们则在最陡下降的方向上降低成本函数。每次下降步幅的大小由参数 α 确定，该参数称为「学习率(Learning Rate)」。


事实上，用于线性回归的代价函数总是一个凸函数(Convex Function)。这样的函数没有局部最优解，只有一个全局最优解。所以我们在使用梯度下降的时候，总会得到一个全局最优解。












[SamirChen]: http://www.samirchen.com "SamirChen"
[1]: {{ page.url }} ({{ page.title }})
[2]: http://www.samirchen.com/notes-of-andrew-ng-machine-learning


